<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS280A Project 3 — Image Warping and Mosaicing</title>

  <link rel="stylesheet" href="../shared.css">
  <link rel="stylesheet" href="./project3.css">
</head>

<body>
  <div class="container">
    <header>
      <h1>CS280A Project 3 — Image Warping and Mosaicing</h1>
      <div class="meta">Name: <strong>Xi Cheng</strong> · Date: <strong>2025-10-08</strong></div>
    </header>

    <!-- ========================== -->
    <!--      TABLE OF CONTENTS     -->
    <!-- ========================== -->
    <nav class="toc">
      <h2>Table of Contents</h2>
      <ul>
        <li><a href="#partA">Part A: Manual Image Mosaicing</a>
          <ul>
            <li><a href="#a1">A.1 Shoot the Pictures</a></li>
            <li><a href="#a2">A.2 Recover Homographies</a></li>
            <li><a href="#a3">A.3 Warp the Images</a></li>
            <li><a href="#a4">A.4 Blend the Images into a Mosaic</a></li>
            <li><a href="#a5">A.5 Bells & Whistles</a></li>
          </ul>
        </li>
    
        <li><a href="#partB">Part B: Automatic Image Mosaicing</a>
          <ul>
            <li><a href="#b1">B.1 Detecting corner features in an image </a></li>
            <li><a href="#b2">B.2 Extracting a Feature Descriptor for each feature point</a></li>
            <li><a href="#b3">B.3 Matching these feature descriptors between two images </a></li>
            <li><a href="#b4">B.4 Use a robust method (RANSAC) to compute and apply homographies </a></li>
            <li><a href="#b5">B.5 Bells & Whistles</a></li>
          </ul>
        </li>
      </ul>
    </nav>


    <!-- ========================== -->
    <!--         PART A START        -->
    <!-- ========================== -->
    <section class="part" id="partA">
      <h2>Part A: Manual Image Mosaicing</h2>
      <p class="lead">
        In this part, I manually select correspondences between images to build a panorama.  
        The pipeline includes five main steps: capturing overlapping images, estimating homographies,  
        warping each image to a reference frame, and blending them into a seamless mosaic.
      </p>

        <!-- A.1 -->
        <div class="subhead" id="a1">A.1 Shoot the Pictures</div>
        <p>
          I captured three sets of overlapping photos by fixing the <strong>center of projection (COP)</strong> 
          and rotating the camera horizontally. This ensures projective (perspective) transforms between images 
          for reliable homography estimation.
        </p>
        
        <ul>
          <li>Avoided fisheye lenses and kept straight lines undistorted.</li>
          <li>Shot close in time to keep lighting and subjects consistent.</li>
          <li>Maintained about <strong>40–70%</strong> overlap between frames.</li>
        </ul>
        
        <p>
          The first two sets were taken outdoors ,  
          and the third set indoors. Each set contains three overlapping photos.
        </p>
        
        <!-- ====== Set 1 ====== -->
        <figure class="center">
          <div class="grid-3">
            <a href="#img1_1"><img src="media/img1_1.jpg" alt="Outdoor Set 1 - Image 1"></a>
            <a href="#img1_2"><img src="media/img1_2.jpg" alt="Outdoor Set 1 - Image 2"></a>
            <a href="#img1_3"><img src="media/img1_3.jpg" alt="Outdoor Set 1 - Image 3"></a>
          </div>
          <figcaption>Set 1 – Outdoor buildings.</figcaption>
        </figure>
        
        <!-- ====== Set 2 ====== -->
        <figure class="center">
          <div class="grid-3">
            <a href="#img2_1"><img src="media/img2_1.jpg" alt="Outdoor Set 2 - Image 1"></a>
            <a href="#img2_2"><img src="media/img2_2.jpg" alt="Outdoor Set 2 - Image 2"></a>
            <a href="#img2_3"><img src="media/img2_3.jpg" alt="Outdoor Set 2 - Image 3"></a>
          </div>
          <figcaption>Set 2 – Outdoor buildings.</figcaption>
        </figure>
        
        <!-- ====== Set 3 ====== -->
        <figure class="center">
          <div class="grid-3">
            <a href="#img3_1"><img src="media/img3_1.jpg" alt="Indoor Set 3 - Image 1"></a>
            <a href="#img3_2"><img src="media/img3_2.jpg" alt="Indoor Set 3 - Image 2"></a>
            <a href="#img3_3"><img src="media/img3_3.jpg" alt="Indoor Set 3 - Image 3"></a>
          </div>
          <figcaption>Set 3 – Indoor scene</figcaption>
        </figure>
        
        <p>
          These sequences provide sufficient parallax-free overlap for computing homographies and creating mosaics in later steps.
        </p>
        
        <!-- ====== Lightbox targets ====== -->
        <!-- Set 1 -->
        <a href="#" class="lightbox" id="img1_1"><img src="media/img1_1.jpg" alt="Outdoor Set 1 enlarged"></a>
        <a href="#" class="lightbox" id="img1_2"><img src="media/img1_2.jpg" alt="Outdoor Set 1 enlarged"></a>
        <a href="#" class="lightbox" id="img1_3"><img src="media/img1_3.jpg" alt="Outdoor Set 1 enlarged"></a>
        <!-- Set 2 -->
        <a href="#" class="lightbox" id="img2_1"><img src="media/img2_1.jpg" alt="Outdoor Set 2 enlarged"></a>
        <a href="#" class="lightbox" id="img2_2"><img src="media/img2_2.jpg" alt="Outdoor Set 2 enlarged"></a>
        <a href="#" class="lightbox" id="img2_3"><img src="media/img2_3.jpg" alt="Outdoor Set 2 enlarged"></a>
        <!-- Set 3 -->
        <a href="#" class="lightbox" id="img3_1"><img src="media/img3_1.jpg" alt="Indoor Set 3 enlarged"></a>
        <a href="#" class="lightbox" id="img3_2"><img src="media/img3_2.jpg" alt="Indoor Set 3 enlarged"></a>
        <a href="#" class="lightbox" id="img3_3"><img src="media/img3_3.jpg" alt="Indoor Set 3 enlarged"></a>
        
        <!-- A.2 -->
        <div class="subhead" id="a2">A.2 Recover Homographies</div>
        
        <p>
          Before aligning and blending the images, we first need to recover the <strong>projective transformation</strong> (homography)
          between each pair of overlapping images.  
          A homography maps points from one image plane to another according to:
        </p>
        
        <div class="formula">
          <span style="font-family:'Times New Roman', serif; font-size:1.05rem;">
            <b>p′ = H&nbsp;p</b>, &nbsp; where H is a 3×3 matrix with 8 degrees of freedom (<i>h</i><sub>33</sub> = 1).
          </span>
        </div>
        
        <p>
          The matrix <code>H</code> is estimated from a set of corresponding points (<i>p</i>, <i>p′</i>) 
          selected manually between two overlapping images.  
          Each correspondence contributes two linear equations, allowing us to solve for the eight unknown entries of H.
        </p>
        
        <h4>Method Overview</h4>
        <p>
          I implemented a function:
        </p>
        
        <div class="formula">
          <code style="font-size:1.05rem;">H = computeH(im1_pts, im2_pts)</code>
        </div>
        
        <p>
          Given <i>n</i> pairs of points (<code>im1_pts</code>, <code>im2_pts</code>), we form a linear system <b>A h = b</b>,  
          where <b>h</b> is a vector of the eight unknowns of H.  
          Each pair <b>p</b> = [x y 1]<sup>T</sup>, <b>p′</b> = [x′ y′ 1]<sup>T</sup> contributes two equations derived from the projective constraint:
        </p>
        
        <div class="formula">
          <span style="font-family:'Times New Roman', serif; font-size:1.05rem; line-height:1.6;">
            x′(h<sub>7</sub>x + h<sub>8</sub>y + 1) = h<sub>1</sub>x + h<sub>2</sub>y + h<sub>3</sub><br>
            y′(h<sub>7</sub>x + h<sub>8</sub>y + 1) = h<sub>4</sub>x + h<sub>5</sub>y + h<sub>6</sub>
          </span>
        </div>
        
        <p>
          With at least 4 correspondences (<i>n ≥ 4</i>), we solve for <b>h</b> using least squares to minimize ‖A h − b‖²:
        </p>
        
        <div class="formula">
          <span style="font-family:'Segoe UI', ui-monospace, monospace; font-size:1.05rem;">
            h = (A<sup>T</sup>A)<sup>−1</sup>A<sup>T</sup>b , &nbsp;&nbsp; then &nbsp; H = reshape([h; 1], 3×3)
          </span>
        </div>
        
        <pre><code class="language-python">def computeH(im1_pts, im2_pts):
            # Build A and b from n correspondences
            for (x, y), (xp, yp) in zip(im1_pts, im2_pts):
                A.append([x, y, 1, 0, 0, 0, -x*xp, -y*xp])
                A.append([0, 0, 0, x, y, 1, -x*yp, -y*yp])
                b.extend([xp, yp])
            # Solve least squares Ah=b
            h, *_ = np.linalg.lstsq(A, b, rcond=None)
            h = np.append(h, 1)
            return h.reshape((3,3))</code></pre>
        
        <h4>Point Correspondences</h4>
        <p>
          I manually selected more than 4 point correspondences 
          and saved them in JSON format.  
          The visualization below shows the correspondences between two overlapping images 
          (blue = points in image 1, green = points in image 2, red lines = matches).
        </p>
        
        <figure class="center">
          <img src="media/A2.png" alt="Point correspondences between two images" style="max-width:850px;">
          <figcaption>Visualized correspondences used for homography estimation.</figcaption>
        </figure>
        
        <h4>Recovered Homography Matrix</h4>
        <p>
          Solving the least-squares system yields the following 3×3 homography matrix:
        </p>
        
        <pre><code class="language-text">Homography from img1_1 → img1_2:
        [[ 1.58520953e+00  8.46559002e-02 -1.52327673e+03]
         [ 1.61001912e-01  1.38435647e+00 -4.29478965e+02]
         [ 1.57146223e-04  2.16525724e-05  1.00000000e+00]]
        </code></pre>
        
        <p>
          Normalizing point coordinates (centering + scaling) before solving greatly improves numerical stability.  
          The recovered <code>H</code> accurately maps planar features between the two views, 
          forming the geometric foundation for warping and mosaicing in later parts.
        </p>

        
        <!-- A.3 -->
        <div class="subhead" id="a3">A.3 Warp the Images</div>
        
        <p>
          With the recovered homographies, we can now <strong>warp each image toward a reference plane</strong>.
          Warping remaps pixel coordinates from the input image to new locations according to:
        </p>
        
        <div class="formula">
          <span style="font-family:'Times New Roman', serif; font-size:1.05rem;">
            p′ ≈ H&nbsp;p &nbsp;&nbsp;&nbsp;&nbsp;⇔&nbsp;&nbsp;&nbsp;&nbsp; p ≈ H<sup>−1</sup>p′
          </span>
        </div>
        
        <p>
          To avoid holes in the output, we use <strong>inverse warping</strong>:  
          for each output pixel <i>p′</i>, compute its source location <i>p = H<sup>−1</sup>p′</i> and interpolate its intensity value.  
          Two interpolation methods were implemented manually:
        </p>
        
        <h4>Interpolation Methods</h4>
        
        <p>
          Two interpolation strategies were implemented <strong>from scratch</strong>, without using any library functions:
        </p>
        
        <ul>
          <li>
            <p>
              <strong>Nearest Neighbor Interpolation</strong>:  
              For each destination pixel (<i>x′, y′</i>), we compute its corresponding point (<i>x, y</i>) in the source image.
              We then <b>round</b> both coordinates to the nearest integer and copy that pixel value directly.  
              This method is computationally very fast (constant time per pixel) but can cause visible artifacts such as
              <em>jagged edges</em> or <em>blocky patterns</em> because it ignores sub-pixel precision.  
              It effectively creates a piecewise-constant approximation of the original intensity field.
            </p>
          </li>
        
          <li>
            <p>
              <strong>Bilinear Interpolation</strong>:  
              Instead of snapping to the nearest pixel, bilinear interpolation considers the <b>four surrounding pixels</b>
              of the computed source coordinate (<i>x, y</i>).  
              It computes the final intensity as a <b>weighted average</b> based on the fractional distances to each corner:  
            </p>
        
            <div class="formula">
              <span style="font-family:'Times New Roman', serif; font-size:1.05rem; line-height:1.6;">
                I(x, y) = I<sub>00</sub>(1−dx)(1−dy) + I<sub>10</sub>dx(1−dy) + I<sub>01</sub>(1−dx)dy + I<sub>11</sub>dxdy
              </span>
            </div>
        
            <p>
              where <i>dx</i> and <i>dy</i> are the fractional offsets within the local 2×2 neighborhood.  
              This method produces much <b>smoother and more continuous intensity transitions</b>,  
              particularly for slanted edges or text regions, at the cost of roughly 3–4× higher computation time.
            </p>
          </li>
        </ul>
        

        
        <h4>Rectification Examples</h4>
        
        <p>
          I tested both interpolation methods on three rectification tasks.
          For first 2 image, I clicked four corners of a rectangular region and mapped them to a perfect square (0,0)–(800,800). And for the last one, I warped the one image to the other which will be showed in A.4.
        </p>
        
        <!-- Example 1 -->
        <figure class="center">
          <div class="grid-1">
            <a href="#A3_1"><img src="media/A3_1.png" alt="Rectification result example 1" style="max-width:900px;"></a>
          </div>
          <figcaption>Example 1</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A3_1">
          <img src="media/A3_1.png" alt="Rectification result example 1 enlarged">
        </a>
        
        <!-- Example 2 -->
        <figure class="center">
          <div class="grid-1">
            <a href="#A3_2"><img src="media/A3_2.png" alt="Rectification result example 2" style="max-width:900px;"></a>
          </div>
          <figcaption>Example 2</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A3_2">
          <img src="media/A3_2.png" alt="Rectification result example 2 enlarged">
        </a>
        
        <!-- Example 3 -->
        <figure class="center">
          <div class="grid-1">
            <a href="#A3_3"><img src="media/A3_3.png" alt="Warp result example 3" style="max-width:900px;"></a>
          </div>
          <figcaption>Example 3</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A3_3">
          <img src="media/A3_3.png" alt="Warp result example 3 enlarged">
        </a>
        
        <h4>Runtime Comparison</h4>
        
        <table>
          <thead>
            <tr>
              <th>Example</th>
              <th>Nearest Neighbor</th>
              <th>Bilinear</th>
              <th>Speed Ratio (Bilinear / NN)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Rectification 1</td>
              <td>0.1471 s</td>
              <td>0.4336 s</td>
              <td>2.95× slower</td>
            </tr>
            <tr>
              <td>Rectification 2</td>
              <td>0.3128 s</td>
              <td>1.2111 s</td>
              <td>3.87× slower</td>
            </tr>
            <tr>
              <td>Warp (img1_1 → img1_2)</td>
              <td>1.3001 s</td>
              <td>4.2479 s</td>
              <td>3.27× slower</td>
            </tr>
          </tbody>
        </table>
        
        <h4>Discussion</h4>
        
        <p>
          From these three examples, we observe that <strong>Nearest Neighbor</strong> interpolation runs approximately 3× faster,  
          but often introduces aliasing and jagged edges.  
          <strong>Bilinear</strong> interpolation produces smoother and more visually accurate results by blending neighboring pixels,  
          at the cost of additional computation.  
          Overall, bilinear interpolation is preferred for high-quality mosaics,  
          while nearest neighbor provides a fast reference for debugging and verification.
        </p>
                
        <!-- A.4 -->
        <div class="subhead" id="a4">A.4 Blend the Images into a Mosaic</div>
        
        <p>
          To build a seamless panorama, I implemented a <strong>homography-based warping and alpha-weighted blending pipeline</strong>.
          The goal is to align all input images into a common reference frame and blend their overlapping regions smoothly 
          to remove visible seams and intensity jumps. The following steps describe the process in detail, with key supporting functions noted.
        </p>
        
        <ol>
          <li>
            <strong>Homography Estimation:</strong>  
            For each pair of adjacent images, I compute a homography <code>H<sub>i→i+1</sub></code> using the normalized DLT algorithm implemented in 
            <code>computeH(im1_pts, im2_pts)</code>.  
            This transformation maps coordinates from one image into the coordinate system of its neighbor.
          </li>
        
          <li>
            <strong>Reference Frame Selection:</strong>  
            The middle image is selected as the reference frame (<code>ref_idx</code>).  
            All other images are transformed into this frame by chaining their pairwise homographies:
            <ul>
              <li>Left of reference: <code>H_to_ref[i] = H_to_ref[i+1] @ H<sub>i→i+1</sub></code></li>
              <li>Right of reference: <code>H_to_ref[i] = H_to_ref[i-1] @ inv(H<sub>i−1→i</sub>)</code></li>
            </ul>
            These transformations are handled by <code>create_multi_image_mosaic()</code>.
          </li>
        
          <li>
            <strong>Canvas Setup:</strong>  
            The function <code>compute_bounding_box()</code> determines the bounding region for each warped image 
            by projecting its four corners using <code>H_to_ref[i]</code>.  
            The global minimum and maximum extents define the size of the mosaic canvas and its coordinate shift.
          </li>
        
          <li>
            <strong>Warping with Alpha Masks:</strong>  
            Each image is inverse-warped into the mosaic coordinate frame, along with a corresponding alpha mask that controls blending.
            Two alpha strategies were tested:
            <ul>
              <li>
                <strong>Hard Alpha (Baseline):</strong>  
                Implemented in <code>warp_with_alpha_hard()</code>, this method assigns α=1 for all valid pixels, producing sharp seams at overlaps.
              </li>
              <li>
                <strong>Smooth Feathered Alpha (Improved):</strong>  
                Implemented in <code>warp_with_alpha_smooth()</code>, this method builds a soft alpha mask using a distance transform:
                <div class="formula" style="margin:.4em 0;">
                  α(x,y) = ( d(x,y) / max d )<sup>γ</sup> with γ ≈ 1.2
                </div>
                where d(x,y) is the distance to the nearest image boundary.  
                The mask value decays smoothly near borders, producing gradual transitions and reducing edge artifacts.
              </li>
            </ul>
          </li>
        
          <li>
            <strong>Progressive Blending:</strong>  
            Each warped image is composited into the global canvas using 
            <code>blend_images_laplacian()</code>, which performs lightweight Laplacian-like blending.  
            This function first applies Gaussian smoothing (<code>G</code>) to both images and their alpha masks, 
            computes Laplacian layers (<code>L = I − G</code>), and fuses them via smoothed alpha weighting:
            <div class="formula" style="margin:.4em 0;">
              I<sub>out</sub> = ( A<sub>1</sub>(G<sub>1</sub>+L<sub>1</sub>) + A<sub>2</sub>(G<sub>2</sub>+L<sub>2</sub>) ) / (A<sub>1</sub>+A<sub>2</sub>)
            </div>
            This ensures consistent exposure and texture continuity across overlapping regions.
          </li>
        
          <li>
            <strong>Alpha Accumulation:</strong>  
            The global alpha mask is updated to record covered regions as new images are blended:
            <div class="formula" style="margin:.4em 0;">
              α<sub>mosaic</sub> ← α<sub>mosaic</sub> + α<sub>new</sub> − α<sub>mosaic</sub> · α<sub>new</sub>
            </div>
            This maintains a smooth cumulative visibility map for the mosaic.
          </li>
        
        </ol>
        
        <p>
          <strong>Main supporting functions:</strong>
          <ul>
            <li><code>computeH</code>: Computes pairwise homographies from matched points.</li>
            <li><code>compute_bounding_box</code>: Finds the projected bounding region of a warped image.</li>
            <li><code>warp_with_alpha_hard</code> / <code>warp_with_alpha_smooth</code>: Performs inverse warping with different α strategies.</li>
            <li><code>blend_images_laplacian</code>: Merges warped images using Gaussian-smoothed α weighting and Laplacian details.</li>
            <li><code>create_multi_image_mosaic</code>: Orchestrates warping, placement, and blending of all images.</li>
          </ul>
        </p>
        
        <p>
          The results show that <strong>smooth feathered alpha blending</strong> 
          significantly reduces seam visibility and exposure mismatches compared to hard alpha blending.  
          The weighted falloff near borders produces continuous, visually pleasing panoramas 
          while retaining sharpness in non-overlapping areas.
        </p>
        
        <!-- Scene Results -->
        <figure class="center">
          <a href="#A4_1_original"><img src="media/A4_1_original.png" alt="Scene 1 originals" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Scene 1 (outdoor): original inputs.</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_1_original"><img src="media/A4_1_original.png" alt="Scene 1 originals enlarged"></a>
        
        <figure class="center">
          <div class="grid-2">
            <a href="#A4_1_hard"><img src="media/A4_1_hard.png" alt="Scene 1 hard alpha"></a>
            <a href="#A4_1_smooth"><img src="media/A4_1_smooth.png" alt="Scene 1 smooth alpha"></a>
          </div>
          <figcaption>Scene 1: hard α (left) vs. smooth α (right).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_1_hard"><img src="media/A4_1_hard.png" alt="Scene 1 hard alpha enlarged"></a>
        <a href="#" class="lightbox" id="A4_1_smooth"><img src="media/A4_1_smooth.png" alt="Scene 1 smooth alpha enlarged"></a>

        <figure class="center">
          <a href="#A4_2_original"><img src="media/A4_2_original.png" alt="Scene 2 originals" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Scene 2 (outdoor): original inputs.</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_2_original"><img src="media/A4_2_original.png" alt="Scene 2 originals enlarged"></a>

        <figure class="center">
          <div class="grid-2">
            <a href="#A4_2_hard"><img src="media/A4_2_hard.png" alt="Scene 2 hard alpha"></a>
            <a href="#A4_2_smooth"><img src="media/A4_2_smooth.png" alt="Scene 2 smooth alpha"></a>
          </div>
          <figcaption>Scene 2: hard α (left) vs. smooth α (right).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_2_hard"><img src="media/A4_2_hard.png" alt="Scene 2 hard alpha enlarged"></a>
        <a href="#" class="lightbox" id="A4_2_smooth"><img src="media/A4_2_smooth.png" alt="Scene 2 smooth alpha enlarged"></a>

        <figure class="center">
          <a href="#A4_3_original"><img src="media/A4_3_original.png" alt="Scene 3 originals" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Scene 3 (indoor): original inputs.</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_3_original"><img src="media/A4_3_original.png" alt="Scene 3 originals enlarged"></a>

        <figure class="center">
          <div class="grid-2">
            <a href="#A4_3_hard"><img src="media/A4_3_hard.png" alt="Scene 3 hard alpha"></a>
            <a href="#A4_3_smooth"><img src="media/A4_3_smooth.png" alt="Scene 3 smooth alpha"></a>
          </div>
          <figcaption>Scene 3: hard α (left) vs. smooth α (right).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A4_3_hard"><img src="media/A4_3_hard.png" alt="Scene 3 hard alpha enlarged"></a>
        <a href="#" class="lightbox" id="A4_3_smooth"><img src="media/A4_3_smooth.png" alt="Scene 3 smooth alpha enlarged"></a>


    
        <!-- A.5 -->
        <div class="subhead" id="a5">A.5 Bells & Whistles</div>
        
        <p>
          As an extension to the planar mosaic, I implemented <strong>cylindrical projection</strong> to better handle wide-angle panoramas.  
          In this approach, each image is first projected onto a cylindrical surface before homography computation and blending.  
          This reduces the perspective distortion that appears when the camera rotates over a large field of view.
        </p>
        
        <ol>
          <li>
            <strong>Cylindrical Projection Mapping:</strong>  
            The key difference from planar mosaicing lies in remapping every pixel from the image plane to cylindrical coordinates.  
            Given a focal length <code>f</code>, and assuming the optical axis points forward, each pixel <code>(x, y)</code> is projected as:
            <div class="formula" style="margin:.5em 0;">
              x′ = f · arctan((x − cₓ) / f) + cₓ,&nbsp;&nbsp;
              y′ = f · (y − cᵧ) / √((x − cₓ)² + f²) + cᵧ
            </div>
            where (<code>cₓ, cᵧ</code>) is the image center.  
            This transformation preserves vertical lines and bends horizontal ones smoothly, producing a natural-looking panorama even for large rotations.
            <br>
            Implemented in:
            <ul>
              <li><code>project_points_to_cylindrical()</code>: projects matched feature points from the original image plane into the cylindrical space.</li>
              <li><code>cylindrical_warp_image()</code>: remaps each image pixel using <code>cv2.remap</code> for efficient inverse warping.</li>
            </ul>
          </li>
        
          <li>
            <strong>Homography Re-Estimation on Cylindrical Domain:</strong>  
            After projection, all corresponding points (previously stored in JSON files) are converted to cylindrical coordinates.  
            The new homographies are then recomputed using <code>computeH()</code> on these projected points, ensuring geometric consistency on the curved surface.
          </li>
        
          <li>
            <strong>Multi-Image Blending:</strong>  
            Once warped, the images are merged using the same blending pipeline introduced in A.4 —  
            <code>warp_with_alpha_smooth()</code> for soft feathered alpha masks and <code>blend_images_laplacian()</code> for smooth exposure transitions.  
            The composition order (<em>left → reference → right</em>) and alpha update formula remain unchanged, handled by <code>create_multi_image_mosaic()</code>.
          </li>
        
          <li>
            <strong>Focal Length Variation:</strong>  
            The effective focal length <code>f</code> determines the curvature of the cylindrical projection.  
            Smaller <code>f</code> values yield stronger curvature (wider field of view but more compression),  
            while larger <code>f</code> values approximate a planar projection.  
            To visualize this effect, I generated mosaics using three focal scales:
            <ul>
              <li><code>focal_scale = 1.0</code> → strong curvature</li>
              <li><code>focal_scale = 1.3</code> → balanced projection</li>
              <li><code>focal_scale = 1.5</code> → nearly planar</li>
            </ul>
            Each case uses the same <code>test_multi_mosaic_cylindrical()</code> function, which handles projection, homography computation, and blending automatically.
          </li>
        </ol>
        
        <p>
          Cylindrical projection effectively preserves scene geometry in wide-angle mosaics,  
          minimizing vertical distortion and preventing the “bowing” artifacts typical of planar projections.  
          As the focal length increases, the panorama gradually transitions back toward the planar case.
        </p>
        
        <!-- Cylindrical Results -->
        <figure class="center">
          <a href="#A5_3_1.0"><img src="media/A5_3_1.0.png" alt="Cylindrical mosaic f=1.0" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Mosaic using cylindrical projection (focal scale = 1.0).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A5_3_1.0"><img src="media/A5_3_1.0.png" alt="Cylindrical mosaic f=1.0 enlarged"></a>
        
        <figure class="center">
          <a href="#A5_3_1.3"><img src="media/A5_3_1.3.png" alt="Cylindrical mosaic f=1.3" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Mosaic using cylindrical projection (focal scale = 1.3).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A5_3_1.3"><img src="media/A5_3_1.3.png" alt="Cylindrical mosaic f=1.3 enlarged"></a>
        
        <figure class="center">
          <a href="#A5_3_1.5"><img src="media/A5_3_1.5.png" alt="Cylindrical mosaic f=1.5" style="max-width:900px; border-radius:8px;"></a>
          <figcaption>Mosaic using cylindrical projection (focal scale = 1.5).</figcaption>
        </figure>
        <a href="#" class="lightbox" id="A5_3_1.5"><img src="media/A5_3_1.5.png" alt="Cylindrical mosaic f=1.5 enlarged"></a>
    </section>
    <!-- Part B Header -->
    <section class="part" id="partB">
      <h2>Part B: Automatic Image Mosaicing</h2>
    
        <!-- B.1 -->
        
         <div class="subhead" id="b1">B.1 Harris Corner Detection</div>
        
        <p>
          To automate feature selection for image matching, we begin by detecting 
          <strong>Harris corners</strong> at a single scale. 
          Harris corners are points with strong intensity variation in orthogonal directions, 
          making them repeatable and stable for matching across images.
        </p>
        
        <p>
          The Harris response function <code>H(x, y)</code> is computed from the local gradient structure tensor, 
          and corner points are selected at local maxima of this response. 
          This typically yields many keypoints clustered in textured areas, which may not be well distributed spatially.
        </p>
        
        <p>
          To address this, we apply <strong>Adaptive Non-Maximal Suppression (ANMS)</strong> 
          to select a fixed number of <em>strong but spatially diverse</em> keypoints.
          ANMS computes the suppression radius for each corner, defined as the distance to the nearest 
          significantly stronger corner, and keeps points with the largest radii. 
          This ensures uniform coverage of the image while retaining strong features.
        </p>
        
        <p>
          The figures below compare Harris corner detection <em>before</em> and <em>after</em> applying ANMS. 
          Harris detects many clustered corners, whereas ANMS spreads keypoints across the image — 
          improving robustness for matching and mosaicing.
        </p>
        <figure class="center grid-2">
          <!-- Before ANMS -->
          <a href="#B1_before">
            <img src="media/B1_1.png" alt="Harris corners before ANMS" style="border-radius:8px;">
          </a>
        
          <!-- After ANMS -->
          <a href="#B1_after">
            <img src="media/B1_2.png" alt="Harris corners after ANMS" style="border-radius:8px;">
          </a>
        
        </figure>
        
        <!-- Lightbox -->
        <a href="#" class="lightbox" id="B1_before">
          <img src="media/B1_1.png" alt="Harris corners before ANMS enlarged">
        </a>
        <a href="#" class="lightbox" id="B1_after">
          <img src="media/B1_2.png" alt="Harris corners after ANMS enlarged">
        </a>

        
        
        
        <!-- B.2 -->
        
        <div class="subhead" id="b2">B.2 Extracting a Feature Descriptor for Each Feature Point</div>
        
        <p>
          After detecting corner features, we extract a fixed-length <strong>feature descriptor</strong> around each selected keypoint.
          The goal is to obtain a stable representation of the local image structure that is robust to changes in illumination and minor geometric variations.
        </p>
        
        <h4>Method Overview</h4>
        <p>
          For each corner, a <code>40×40</code> window centered at the keypoint is sampled, then 
          <strong>downsampled</strong> to an <code>8×8</code> patch to form the descriptor.
          Before flattening, we apply <strong>bias/gain normalization</strong> (zero mean, unit variance) 
          to make the descriptor illumination invariant. 
          Rotation invariance is not considered in this assignment.
        </p>
        
        <p>
          This simple axis-aligned descriptor provides a compact representation that 
          can be matched across images using distance-based methods. 
          The figure below shows several sampled points and their corresponding 8×8 descriptors.
        </p>
        

        <figure class="center">
          <a href="#B2_points">
            <img src="media/B2_1.png" alt="Selected feature points" style="max-width:500; border-radius:8px;">
          </a>
        </figure>
        <a href="#" class="lightbox" id="B2_points">
          <img src="media/B2_1.png" alt="Selected feature points enlarged">
        </a>
        
        <figure class="center">
          <a href="#B2_descriptors">
            <img src="media/B2_2.png" alt="Extracted feature descriptors" style="max-width:850px; border-radius:8px;">
          </a>
        </figure>
        <a href="#" class="lightbox" id="B2_descriptors">
          <img src="media/B2_2.png" alt="Extracted feature descriptors enlarged">
        </a>


        <!--   B.3 -->
        
        <div class="subhead" id="b3">B.3 Matching Feature Descriptors Between Two Images</div>
        
        <p>
          Once we have extracted normalized 8×8 descriptors for each detected corner,
          we need to establish <strong>correspondences</strong> between features from different images.
          The goal is to identify pairs of points that represent the same physical location in the scene.
        </p>
        
        <h4>Method Overview</h4>
        <p>
          We implement a simple and effective matching strategy using 
          <strong>Lowe’s ratio test</strong> (Section 5 of the paper).  
          For each descriptor in the first image, we compute its Euclidean distance to all descriptors 
          in the second image and find the two nearest neighbors.  
          A match is accepted if the ratio between the best and second-best distances is below a threshold 
          (here, <code>0.8</code>).  
          This rejects ambiguous matches and keeps only distinctive correspondences.
        </p>
        
        <p>
          The figure below shows matched keypoints between two images after applying the ratio test.
          The yellow lines indicate correspondences, with red and green dots marking matched keypoints 
          in the first and second image respectively.  
          These correspondences form the basis for robust <strong>homography estimation</strong> in the next step.
        </p>
        
        
        <figure class="center">
          <a href="#B3_matches">
            <img src="media/B3_matches.png" alt="Matched feature pairs between two images" style="max-width:900px; border-radius:8px;">
          </a>
        </figure>
        <a href="#" class="lightbox" id="B3_matches">
          <img src="media/B3_matches.png" alt="Matched feature pairs enlarged">
        </a>

        

        <!--    B.4    -->
        
        <div class="subhead" id="b4">B.4 Robust Homography Estimation with RANSAC</div>
        
        <p>
          Even after feature matching, some correspondences are incorrect or noisy, 
          which can significantly degrade homography estimation.  
          To handle this, I implemented <strong>4-point RANSAC</strong> to robustly estimate 
          the homography between adjacent image pairs.
        </p>
        
        <h4>Method Overview</h4>
        <p>
          In each iteration, four random matches are sampled to compute a candidate homography using <code>computeH()</code>.  
          All feature points are then projected, and the reprojection error is measured.  
          Points with error below a fixed threshold are considered inliers.  
          The model with the maximum number of inliers is selected, and a final homography is 
          recomputed using all inliers.
        </p>
        
        <p>
          To evaluate the quality of the automatic stitching pipeline, I used the <strong>same input images</strong> 
          as in Part A (manual homography) and directly compared the resulting panoramas.
          The figures below show side-by-side comparisons of <strong>manual vs automatic</strong> stitching results 
          across three examples.
        </p>
        

        
        <!-- Example 1 -->
        <div class="grid-2">
          <figure>
            <a href="#A4_1_smooth"><img src="media/A4_1_smooth.png"></a>
            <figcaption>Manual mosaic </figcaption>
          </figure>
          <figure>
            <a href="#B4_1"><img src="media/B4_1.png"></a>
            <figcaption>Auto mosaic </figcaption>
          </figure>
        </div>


        <a href="#" class="lightbox" id="A4_1_smooth">
          <img src="media/A4_1_smooth.png" alt="Manual mosaic example 1 enlarged">
        </a>
        <a href="#" class="lightbox" id="B4_1">
          <img src="media/B4_1.png" alt="Automatic mosaic example 1 enlarged">
        </a>
        
        <!-- Example 2 -->
        <div class="grid-2">
          <figure>
            <a href="#A4_2_smooth"><img src="media/A4_2_smooth.png"></a>
            <figcaption>Manual mosaic </figcaption>
          </figure>
          <figure>
            <a href="#B4_2"><img src="media/B4_2.png"></a>
            <figcaption>Auto mosaic </figcaption>
          </figure>
        </div>
        <a href="#" class="lightbox" id="A4_2_smooth">
          <img src="media/A4_2_smooth.png" alt="Manual mosaic example 2 enlarged">
        </a>
        <a href="#" class="lightbox" id="B4_2">
          <img src="media/B4_2.png" alt="Automatic mosaic example 2 enlarged">
        </a>
        
        <!-- Example 3 -->
        <div class="grid-2">
          <figure>
            <a href="#A4_3_smooth"><img src="media/A4_3_smooth.png"></a>
            <figcaption>Manual mosaic </figcaption>
          </figure>
          <figure>
            <a href="#B4_3"><img src="media/B4_3.png"></a>
            <figcaption>Auto mosaic </figcaption>
          </figure>
        </div>
        <a href="#" class="lightbox" id="A4_3_smooth">
          <img src="media/A4_3_smooth.png" alt="Manual mosaic example 3 enlarged">
        </a>
        <a href="#" class="lightbox" id="B4_3">
          <img src="media/B4_3.png" alt="Automatic mosaic example 3 enlarged">
        </a>
        
        
                        
        <!--   B.5  -->
        
        <div class="subhead" id="b5">B.5 Bells & Whistles — Multiscale Processing</div>
        
        <p>
          In the previous sections, both Harris corner detection and descriptor extraction were performed at a single image scale. 
          A single fixed scale may either miss small features or produce poorly localized keypoints on large structures.
        </p>
        
        <p>
          To address this, I implemented <strong>multiscale processing</strong> for both keypoint detection and feature description.
          This approach improves keypoint coverage, matching stability, and overall mosaic quality, especially for scenes with significant scale variation.
        </p>
        
        <h4>Method Overview</h4>
        
        <ul>
          <li>
            <strong>1. Multiscale Harris Corner Detection</strong>  
            I defined a set of scales <code>[1.0, 0.5, 0.25]</code>. For each scale, 
            the image is resized accordingly and Harris corner detection is applied.
            Top corners (by response strength) are selected at each scale and mapped back to the original image coordinates.
            Finally, <strong>Adaptive Non-Maximal Suppression (ANMS)</strong> is applied across all scales to keep the most spatially diverse and informative keypoints.  
            This is implemented in <code>get_multiscale_corners_with_scales()</code>.
          </li>
        
          <li>
            <strong>2. Scale-Aware Feature Descriptor Extraction</strong>  
            In <code>get_features_multiscale()</code>, each keypoint is described at the scale where it was detected. 
            Specifically, a <code>40×40</code> patch is sampled around the keypoint on the scaled image, 
            Gaussian blurred, and then downsampled to <code>8×8</code> to form a normalized descriptor.
            This ensures that both small and large features are represented at their appropriate resolution,
            improving matching precision across scales.
          </li>
        
          <li>
            <strong>3. Matching & RANSAC Mosaic</strong>  
            After multiscale keypoints and descriptors are obtained, the same feature matching pipeline 
            (Lowe’s ratio test + 4-point RANSAC) is applied to estimate homographies between images.
            The resulting mosaics are blended using the same alpha blending and Laplacian pyramid blending as in Part A.
          </li>
        </ul>
        
        <p>
          The visualization below shows the detected Harris corners across scales. Different colors indicate different detection scales.
        </p>
        
        
        <figure class="center">
          <a href="#B5_1">
            <img src="media/B5_1.png" alt="Multiscale Harris corners visualization" style="max-width:900px; border-radius:8px;">
          </a>
          <figcaption>
            Multiscale Harris corners after ANMS. Different colors correspond to different detection scales.
          </figcaption>
        </figure>
        <a href="#" class="lightbox" id="B5_1">
          <img src="media/B5_1.png" alt="Multiscale Harris corners enlarged">
        </a>
        
        <p>
          Compared to the single-scale approach used in B.1–B.4, 
          this multiscale method improves both the <strong>quantity</strong> and <strong>quality</strong> of feature correspondences, 
          particularly in challenging wide-angle or high-texture scenes.
        </p>
        
        <h4>Mosaic Results</h4>
        <p>
          Below are three automatic mosaics generated using multiscale keypoints and descriptors, 
          compared against the previous single-scale mosaics from B.4.  
          Notice that the <strong>ghosting and misalignment</strong> seen in earlier results are significantly reduced, 
          especially in the first and second examples, where large and small features coexist in the scene.
        </p>
        
        <!-- Example 1 -->
        <div class="grid-2">
          <figure>
            <a href="#B4_1"><img src="media/B4_1.png"></a>
            <figcaption>Single scale </figcaption>
          </figure>
          <figure>
            <a href="#B5_mosaic1"><img src="media/B5_mosaic1.png"></a>
            <figcaption>Multiscale </figcaption>
          </figure>
        </div>
        <a href="#" class="lightbox" id="B4_1">
          <img src="media/B4_1.png" alt="Single scale mosaic 1 enlarged">
        </a>
        <a href="#" class="lightbox" id="B5_mosaic1">
          <img src="media/B5_mosaic1.png" alt="Multiscale mosaic 1 enlarged">
        </a>
        
        <!-- Example 2 -->
        <div class="grid-2">
          <figure>
            <a href="#B4_2"><img src="media/B4_2.png"></a>
            <figcaption>Single scale </figcaption>
          </figure>
          <figure>
            <a href="#B5_mosaic2"><img src="media/B5_mosaic2.png"></a>
            <figcaption>Multiscale </figcaption>
          </figure>
        </div>
        <a href="#" class="lightbox" id="B4_2">
          <img src="media/B4_2.png" alt="Single scale mosaic 2 enlarged">
        </a>
        <a href="#" class="lightbox" id="B5_mosaic2">
          <img src="media/B5_mosaic2.png" alt="Multiscale mosaic 2 enlarged">
        </a>
        
        <!-- Example 3 -->
        <div class="grid-2">
          <figure>
            <a href="#B4_3"><img src="media/B4_3.png"></a>
            <figcaption>Single scale </figcaption>
          </figure>
          <figure>
            <a href="#B5_mosaic3"><img src="media/B5_mosaic3.png"></a>
            <figcaption>Multiscale </figcaption>
          </figure>
        </div>
        <a href="#" class="lightbox" id="B4_3">
          <img src="media/B4_3.png" alt="Single scale mosaic 3 enlarged">
        </a>
        <a href="#" class="lightbox" id="B5_mosaic3">
          <img src="media/B5_mosaic3.png" alt="Multiscale mosaic 3 enlarged">
        </a>
        
        <h4>Analysis</h4>
        <p>
          The multiscale pipeline improves mosaic quality by:
        </p>
        
        <ul>
          <li>Capturing both coarse and fine image structures through multiple scale levels</li>
          <li>Improving feature localization at the appropriate scale of each structure</li>
          <li>Providing more reliable matches for RANSAC, resulting in better homography estimation</li>
        </ul>
        
        <p>
          Overall, multiscale feature processing leads to more stable and visually pleasing panoramas, 
          especially in real-world outdoor images with rich structure at multiple scales.
        </p>


        
    </section>

    <p class="footer">This page is responsive and print-friendly.</p>


      
  </div>



</body>
</html>

